11/22/2023 00:30:45 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False
11/22/2023 00:30:45 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/runs/Nov22_00-28-36_node03,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=./checkpoints,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./checkpoints,
save_on_each_node=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the module from /home/fuyali/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126 (last modified on Tue Nov 21 18:17:12 2023) since it couldn't be found locally at wikitext., or remotely on the Hugging Face Hub.
11/22/2023 00:32:25 - WARNING - datasets.load - Using the latest cached version of the module from /home/fuyali/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126 (last modified on Tue Nov 21 18:17:12 2023) since it couldn't be found locally at wikitext., or remotely on the Hugging Face Hub.
Loading Dataset Infos from /home/fuyali/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126
11/22/2023 00:32:25 - INFO - datasets.info - Loading Dataset Infos from /home/fuyali/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126
Overwrite dataset info from restored data version if exists.
11/22/2023 00:32:25 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126
11/22/2023 00:32:25 - INFO - datasets.info - Loading Dataset info from /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126
Found cached dataset wikitext (/home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
11/22/2023 00:32:26 - INFO - datasets.builder - Found cached dataset wikitext (/home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
Loading Dataset info from /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126
11/22/2023 00:32:26 - INFO - datasets.info - Loading Dataset info from /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126
[INFO|configuration_utils.py:717] 2023-11-22 00:32:36,469 >> loading configuration file config.json from cache at /home/fuyali/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json
[INFO|configuration_utils.py:777] 2023-11-22 00:32:36,470 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.35.2",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_auto.py:566] 2023-11-22 00:32:46,503 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:717] 2023-11-22 00:32:56,536 >> loading configuration file config.json from cache at /home/fuyali/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json
[INFO|configuration_utils.py:777] 2023-11-22 00:32:56,537 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.35.2",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2022] 2023-11-22 00:33:16,678 >> loading file vocab.json from cache at /home/fuyali/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/vocab.json
[INFO|tokenization_utils_base.py:2022] 2023-11-22 00:33:16,678 >> loading file merges.txt from cache at /home/fuyali/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/merges.txt
[INFO|tokenization_utils_base.py:2022] 2023-11-22 00:33:16,678 >> loading file tokenizer.json from cache at /home/fuyali/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2023-11-22 00:33:16,678 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2022] 2023-11-22 00:33:16,678 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2022] 2023-11-22 00:33:16,678 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:717] 2023-11-22 00:33:16,679 >> loading configuration file config.json from cache at /home/fuyali/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json
[INFO|configuration_utils.py:777] 2023-11-22 00:33:16,679 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.35.2",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:3121] 2023-11-22 00:33:16,793 >> loading weights file model.safetensors from cache at /home/fuyali/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/model.safetensors
[INFO|configuration_utils.py:791] 2023-11-22 00:33:18,341 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:3950] 2023-11-22 00:33:21,123 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:3958] 2023-11-22 00:33:21,124 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:751] 2023-11-22 00:33:31,144 >> loading configuration file generation_config.json from cache at /home/fuyali/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/generation_config.json
[INFO|configuration_utils.py:791] 2023-11-22 00:33:31,144 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Loading cached processed dataset at /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-cf6453a935729675.arrow
11/22/2023 00:33:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-cf6453a935729675.arrow
Loading cached processed dataset at /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f3b262cdd31dd2f6.arrow
11/22/2023 00:33:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f3b262cdd31dd2f6.arrow
Loading cached processed dataset at /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7d323309cbc0013b.arrow
11/22/2023 00:33:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7d323309cbc0013b.arrow
Loading cached processed dataset at /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-e18288cbdcb3c0ef.arrow
11/22/2023 00:33:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-e18288cbdcb3c0ef.arrow
Loading cached processed dataset at /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-578e99c1db25e368.arrow
11/22/2023 00:33:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-578e99c1db25e368.arrow
Loading cached processed dataset at /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-cd14e9b19b4087d2.arrow
11/22/2023 00:33:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/fuyali/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-cd14e9b19b4087d2.arrow
11/22/2023 00:36:56 - WARNING - evaluate.loading - Using the latest cached version of the module from /home/fuyali/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Tue Nov 21 18:19:33 2023) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
11/22/2023 00:36:56 - WARNING - accelerate.utils.other - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:1723] 2023-11-22 00:37:05,889 >> ***** Running training *****
[INFO|trainer.py:1724] 2023-11-22 00:37:05,889 >>   Num examples = 2,318
[INFO|trainer.py:1725] 2023-11-22 00:37:05,889 >>   Num Epochs = 1
[INFO|trainer.py:1726] 2023-11-22 00:37:05,889 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1728] 2023-11-22 00:37:05,889 >>   Training with DataParallel so batch size has been adjusted to: 32
[INFO|trainer.py:1729] 2023-11-22 00:37:05,889 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1730] 2023-11-22 00:37:05,889 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1731] 2023-11-22 00:37:05,889 >>   Total optimization steps = 73
[INFO|trainer.py:1732] 2023-11-22 00:37:05,890 >>   Number of trainable parameters = 124,439,808
[INFO|integration_utils.py:718] 2023-11-22 00:37:05,890 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: 161487824. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /fs1/private/user/fuyali/gpt2_test/wandb/run-20231122_003707-c0ox9krr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-pine-3
wandb: ⭐️ View project at https://wandb.ai/161487824/huggingface
wandb: 🚀 View run at https://wandb.ai/161487824/huggingface/runs/c0ox9krr
  0%|          | 0/73 [00:00<?, ?it/s]/home/fuyali/miniconda3/envs/gpt2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|▏         | 1/73 [00:13<16:06, 13.43s/it]                                                1%|▏         | 1/73 [00:13<16:06, 13.43s/it]  3%|▎         | 2/73 [00:17<09:42,  8.20s/it]                                                3%|▎         | 2/73 [00:17<09:42,  8.20s/it]  4%|▍         | 3/73 [00:22<07:37,  6.53s/it]                                                4%|▍         | 3/73 [00:22<07:37,  6.53s/it]  5%|▌         | 4/73 [00:27<06:36,  5.75s/it]                                                5%|▌         | 4/73 [00:27<06:36,  5.75s/it]  7%|▋         | 5/73 [00:31<06:01,  5.31s/it]                                                7%|▋         | 5/73 [00:31<06:01,  5.31s/it]  8%|▊         | 6/73 [00:36<05:38,  5.05s/it]                                                8%|▊         | 6/73 [00:36<05:38,  5.05s/it] 10%|▉         | 7/73 [00:40<05:22,  4.88s/it]                                               10%|▉         | 7/73 [00:40<05:22,  4.88s/it] 11%|█         | 8/73 [00:45<05:10,  4.78s/it]                                               11%|█         | 8/73 [00:45<05:10,  4.78s/it] 12%|█▏        | 9/73 [00:49<05:01,  4.70s/it]                                               12%|█▏        | 9/73 [00:49<05:01,  4.70s/it] 14%|█▎        | 10/73 [00:54<04:53,  4.66s/it]                                                14%|█▎        | 10/73 [00:54<04:53,  4.66s/it] 15%|█▌        | 11/73 [00:58<04:47,  4.64s/it]                                                15%|█▌        | 11/73 [00:58<04:47,  4.64s/it] 16%|█▋        | 12/73 [01:03<04:41,  4.61s/it]                                                16%|█▋        | 12/73 [01:03<04:41,  4.61s/it] 18%|█▊        | 13/73 [01:08<04:35,  4.59s/it]                                                18%|█▊        | 13/73 [01:08<04:35,  4.59s/it] 19%|█▉        | 14/73 [01:12<04:30,  4.58s/it]                                                19%|█▉        | 14/73 [01:12<04:30,  4.58s/it] 21%|██        | 15/73 [01:17<04:25,  4.57s/it]                                                21%|██        | 15/73 [01:17<04:25,  4.57s/it] 22%|██▏       | 16/73 [01:21<04:20,  4.57s/it]                                                22%|██▏       | 16/73 [01:21<04:20,  4.57s/it] 23%|██▎       | 17/73 [01:26<04:15,  4.57s/it]                                                23%|██▎       | 17/73 [01:26<04:15,  4.57s/it] 25%|██▍       | 18/73 [01:30<04:10,  4.56s/it]                                                25%|██▍       | 18/73 [01:30<04:10,  4.56s/it] 26%|██▌       | 19/73 [01:35<04:06,  4.56s/it]                                                26%|██▌       | 19/73 [01:35<04:06,  4.56s/it] 27%|██▋       | 20/73 [01:39<04:02,  4.57s/it]                                                27%|██▋       | 20/73 [01:39<04:02,  4.57s/it] 29%|██▉       | 21/73 [01:44<03:57,  4.56s/it]                                                29%|██▉       | 21/73 [01:44<03:57,  4.56s/it] 30%|███       | 22/73 [01:49<03:52,  4.56s/it]                                                30%|███       | 22/73 [01:49<03:52,  4.56s/it] 32%|███▏      | 23/73 [01:53<03:47,  4.56s/it]                                                32%|███▏      | 23/73 [01:53<03:47,  4.56s/it] 33%|███▎      | 24/73 [01:58<03:43,  4.56s/it]                                                33%|███▎      | 24/73 [01:58<03:43,  4.56s/it] 34%|███▍      | 25/73 [02:02<03:38,  4.56s/it]                                                34%|███▍      | 25/73 [02:02<03:38,  4.56s/it] 36%|███▌      | 26/73 [02:07<03:34,  4.56s/it]                                                36%|███▌      | 26/73 [02:07<03:34,  4.56s/it] 37%|███▋      | 27/73 [02:11<03:29,  4.56s/it]                                                37%|███▋      | 27/73 [02:11<03:29,  4.56s/it] 38%|███▊      | 28/73 [02:16<03:25,  4.56s/it]                                                38%|███▊      | 28/73 [02:16<03:25,  4.56s/it] 40%|███▉      | 29/73 [02:20<03:21,  4.57s/it]                                                40%|███▉      | 29/73 [02:21<03:21,  4.57s/it] 41%|████      | 30/73 [02:25<03:16,  4.57s/it]                                                41%|████      | 30/73 [02:25<03:16,  4.57s/it] 42%|████▏     | 31/73 [02:30<03:11,  4.57s/it]                                                42%|████▏     | 31/73 [02:30<03:11,  4.57s/it] 44%|████▍     | 32/73 [02:34<03:07,  4.56s/it]                                                44%|████▍     | 32/73 [02:34<03:07,  4.56s/it] 45%|████▌     | 33/73 [02:39<03:02,  4.56s/it]                                                45%|████▌     | 33/73 [02:39<03:02,  4.56s/it] 47%|████▋     | 34/73 [02:43<02:57,  4.56s/it]                                                47%|████▋     | 34/73 [02:43<02:57,  4.56s/it] 48%|████▊     | 35/73 [02:48<02:53,  4.57s/it]                                                48%|████▊     | 35/73 [02:48<02:53,  4.57s/it] 49%|████▉     | 36/73 [02:52<02:48,  4.57s/it]                                                49%|████▉     | 36/73 [02:52<02:48,  4.57s/it] 51%|█████     | 37/73 [02:57<02:44,  4.57s/it]                                                51%|█████     | 37/73 [02:57<02:44,  4.57s/it] 52%|█████▏    | 38/73 [03:02<02:41,  4.61s/it]                                                52%|█████▏    | 38/73 [03:02<02:41,  4.61s/it] 53%|█████▎    | 39/73 [03:06<02:36,  4.60s/it]                                                53%|█████▎    | 39/73 [03:06<02:36,  4.60s/it] 55%|█████▍    | 40/73 [03:11<02:31,  4.58s/it]                                                55%|█████▍    | 40/73 [03:11<02:31,  4.58s/it] 56%|█████▌    | 41/73 [03:15<02:26,  4.57s/it]                                                56%|█████▌    | 41/73 [03:15<02:26,  4.57s/it] 58%|█████▊    | 42/73 [03:20<02:21,  4.57s/it]                                                58%|█████▊    | 42/73 [03:20<02:21,  4.57s/it] 59%|█████▉    | 43/73 [03:25<02:17,  4.57s/it]                                                59%|█████▉    | 43/73 [03:25<02:17,  4.57s/it] 60%|██████    | 44/73 [03:29<02:12,  4.57s/it]                                                60%|██████    | 44/73 [03:29<02:12,  4.57s/it] 62%|██████▏   | 45/73 [03:34<02:07,  4.57s/it]                                                62%|██████▏   | 45/73 [03:34<02:07,  4.57s/it] 63%|██████▎   | 46/73 [03:38<02:03,  4.57s/it]                                                63%|██████▎   | 46/73 [03:38<02:03,  4.57s/it] 64%|██████▍   | 47/73 [03:43<01:58,  4.57s/it]                                                64%|██████▍   | 47/73 [03:43<01:58,  4.57s/it] 66%|██████▌   | 48/73 [03:47<01:54,  4.56s/it]                                                66%|██████▌   | 48/73 [03:47<01:54,  4.56s/it] 67%|██████▋   | 49/73 [03:52<01:49,  4.56s/it]                                                67%|██████▋   | 49/73 [03:52<01:49,  4.56s/it] 68%|██████▊   | 50/73 [03:57<01:45,  4.58s/it]                                                68%|██████▊   | 50/73 [03:57<01:45,  4.58s/it] 70%|██████▉   | 51/73 [04:01<01:40,  4.57s/it]                                                70%|██████▉   | 51/73 [04:01<01:40,  4.57s/it] 71%|███████   | 52/73 [04:06<01:35,  4.57s/it]                                                71%|███████   | 52/73 [04:06<01:35,  4.57s/it] 73%|███████▎  | 53/73 [04:10<01:31,  4.56s/it]                                                73%|███████▎  | 53/73 [04:10<01:31,  4.56s/it] 74%|███████▍  | 54/73 [04:15<01:26,  4.57s/it]                                                74%|███████▍  | 54/73 [04:15<01:26,  4.57s/it] 75%|███████▌  | 55/73 [04:19<01:22,  4.56s/it]                                                75%|███████▌  | 55/73 [04:19<01:22,  4.56s/it] 77%|███████▋  | 56/73 [04:24<01:17,  4.56s/it]                                                77%|███████▋  | 56/73 [04:24<01:17,  4.56s/it] 78%|███████▊  | 57/73 [04:28<01:12,  4.56s/it]                                                78%|███████▊  | 57/73 [04:28<01:12,  4.56s/it] 79%|███████▉  | 58/73 [04:33<01:08,  4.56s/it]                                                79%|███████▉  | 58/73 [04:33<01:08,  4.56s/it] 81%|████████  | 59/73 [04:38<01:03,  4.57s/it]                                                81%|████████  | 59/73 [04:38<01:03,  4.57s/it] 82%|████████▏ | 60/73 [04:42<00:59,  4.57s/it]                                                82%|████████▏ | 60/73 [04:42<00:59,  4.57s/it] 84%|████████▎ | 61/73 [04:47<00:54,  4.57s/it]                                                84%|████████▎ | 61/73 [04:47<00:54,  4.57s/it] 85%|████████▍ | 62/73 [04:51<00:50,  4.56s/it]                                                85%|████████▍ | 62/73 [04:51<00:50,  4.56s/it] 86%|████████▋ | 63/73 [04:56<00:45,  4.56s/it]                                                86%|████████▋ | 63/73 [04:56<00:45,  4.56s/it] 88%|████████▊ | 64/73 [05:00<00:41,  4.56s/it]                                                88%|████████▊ | 64/73 [05:00<00:41,  4.56s/it] 89%|████████▉ | 65/73 [05:05<00:36,  4.57s/it]                                                89%|████████▉ | 65/73 [05:05<00:36,  4.57s/it] 90%|█████████ | 66/73 [05:10<00:31,  4.56s/it]                                                90%|█████████ | 66/73 [05:10<00:31,  4.56s/it] 92%|█████████▏| 67/73 [05:14<00:27,  4.56s/it]                                                92%|█████████▏| 67/73 [05:14<00:27,  4.56s/it] 93%|█████████▎| 68/73 [05:19<00:22,  4.57s/it]                                                93%|█████████▎| 68/73 [05:19<00:22,  4.57s/it] 95%|█████████▍| 69/73 [05:23<00:18,  4.57s/it]                                                95%|█████████▍| 69/73 [05:23<00:18,  4.57s/it] 96%|█████████▌| 70/73 [05:28<00:13,  4.56s/it]                                                96%|█████████▌| 70/73 [05:28<00:13,  4.56s/it] 97%|█████████▋| 71/73 [05:32<00:09,  4.56s/it]                                                97%|█████████▋| 71/73 [05:32<00:09,  4.56s/it] 99%|█████████▊| 72/73 [05:37<00:04,  4.56s/it]                                                99%|█████████▊| 72/73 [05:37<00:04,  4.56s/it]100%|██████████| 73/73 [05:39<00:00,  3.79s/it]                                               100%|██████████| 73/73 [05:39<00:00,  3.79s/it][INFO|trainer.py:1955] 2023-11-22 00:42:52,792 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 73/73 [05:39<00:00,  3.79s/it]100%|██████████| 73/73 [05:39<00:00,  4.65s/it]
[INFO|trainer.py:2881] 2023-11-22 00:42:52,794 >> Saving model checkpoint to ./checkpoints
[INFO|configuration_utils.py:461] 2023-11-22 00:42:52,797 >> Configuration saved in ./checkpoints/config.json
[INFO|configuration_utils.py:564] 2023-11-22 00:42:52,798 >> Configuration saved in ./checkpoints/generation_config.json
[INFO|modeling_utils.py:2193] 2023-11-22 00:42:53,574 >> Model weights saved in ./checkpoints/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2023-11-22 00:42:53,576 >> tokenizer config file saved in ./checkpoints/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2023-11-22 00:42:53,577 >> Special tokens file saved in ./checkpoints/special_tokens_map.json
{'loss': 3.7657, 'learning_rate': 4.9315068493150684e-05, 'epoch': 0.01}
{'loss': 3.6904, 'learning_rate': 4.863013698630137e-05, 'epoch': 0.03}
{'loss': 3.561, 'learning_rate': 4.794520547945205e-05, 'epoch': 0.04}
{'loss': 3.5779, 'learning_rate': 4.726027397260274e-05, 'epoch': 0.05}
{'loss': 3.5566, 'learning_rate': 4.657534246575342e-05, 'epoch': 0.07}
{'loss': 3.4159, 'learning_rate': 4.589041095890411e-05, 'epoch': 0.08}
{'loss': 3.5271, 'learning_rate': 4.520547945205479e-05, 'epoch': 0.1}
{'loss': 3.5503, 'learning_rate': 4.452054794520548e-05, 'epoch': 0.11}
{'loss': 3.4438, 'learning_rate': 4.383561643835617e-05, 'epoch': 0.12}
{'loss': 3.429, 'learning_rate': 4.3150684931506855e-05, 'epoch': 0.14}
{'loss': 3.3692, 'learning_rate': 4.2465753424657536e-05, 'epoch': 0.15}
{'loss': 3.4256, 'learning_rate': 4.1780821917808224e-05, 'epoch': 0.16}
{'loss': 3.4032, 'learning_rate': 4.1095890410958905e-05, 'epoch': 0.18}
{'loss': 3.3179, 'learning_rate': 4.041095890410959e-05, 'epoch': 0.19}
{'loss': 3.3994, 'learning_rate': 3.9726027397260274e-05, 'epoch': 0.21}
{'loss': 3.313, 'learning_rate': 3.904109589041096e-05, 'epoch': 0.22}
{'loss': 3.3134, 'learning_rate': 3.8356164383561644e-05, 'epoch': 0.23}
{'loss': 3.3672, 'learning_rate': 3.767123287671233e-05, 'epoch': 0.25}
{'loss': 3.3962, 'learning_rate': 3.698630136986301e-05, 'epoch': 0.26}
{'loss': 3.3106, 'learning_rate': 3.63013698630137e-05, 'epoch': 0.27}
{'loss': 3.4125, 'learning_rate': 3.561643835616438e-05, 'epoch': 0.29}
{'loss': 3.3305, 'learning_rate': 3.493150684931507e-05, 'epoch': 0.3}
{'loss': 3.3822, 'learning_rate': 3.424657534246575e-05, 'epoch': 0.32}
{'loss': 3.305, 'learning_rate': 3.356164383561644e-05, 'epoch': 0.33}
{'loss': 3.2967, 'learning_rate': 3.287671232876712e-05, 'epoch': 0.34}
{'loss': 3.286, 'learning_rate': 3.219178082191781e-05, 'epoch': 0.36}
{'loss': 3.3762, 'learning_rate': 3.1506849315068496e-05, 'epoch': 0.37}
{'loss': 3.3888, 'learning_rate': 3.082191780821918e-05, 'epoch': 0.38}
{'loss': 3.2947, 'learning_rate': 3.0136986301369862e-05, 'epoch': 0.4}
{'loss': 3.3479, 'learning_rate': 2.945205479452055e-05, 'epoch': 0.41}
{'loss': 3.318, 'learning_rate': 2.8767123287671234e-05, 'epoch': 0.42}
{'loss': 3.3364, 'learning_rate': 2.808219178082192e-05, 'epoch': 0.44}
{'loss': 3.3229, 'learning_rate': 2.7397260273972603e-05, 'epoch': 0.45}
{'loss': 3.3312, 'learning_rate': 2.671232876712329e-05, 'epoch': 0.47}
{'loss': 3.3491, 'learning_rate': 2.6027397260273973e-05, 'epoch': 0.48}
{'loss': 3.3526, 'learning_rate': 2.534246575342466e-05, 'epoch': 0.49}
{'loss': 3.3733, 'learning_rate': 2.4657534246575342e-05, 'epoch': 0.51}
{'loss': 3.3227, 'learning_rate': 2.3972602739726026e-05, 'epoch': 0.52}
{'loss': 3.2607, 'learning_rate': 2.328767123287671e-05, 'epoch': 0.53}
{'loss': 3.2716, 'learning_rate': 2.2602739726027396e-05, 'epoch': 0.55}
{'loss': 3.2407, 'learning_rate': 2.1917808219178083e-05, 'epoch': 0.56}
{'loss': 3.1712, 'learning_rate': 2.1232876712328768e-05, 'epoch': 0.58}
{'loss': 3.3271, 'learning_rate': 2.0547945205479453e-05, 'epoch': 0.59}
{'loss': 3.3013, 'learning_rate': 1.9863013698630137e-05, 'epoch': 0.6}
{'loss': 3.3595, 'learning_rate': 1.9178082191780822e-05, 'epoch': 0.62}
{'loss': 3.2126, 'learning_rate': 1.8493150684931506e-05, 'epoch': 0.63}
{'loss': 3.3049, 'learning_rate': 1.780821917808219e-05, 'epoch': 0.64}
{'loss': 3.2753, 'learning_rate': 1.7123287671232875e-05, 'epoch': 0.66}
{'loss': 3.1393, 'learning_rate': 1.643835616438356e-05, 'epoch': 0.67}
{'loss': 3.2484, 'learning_rate': 1.5753424657534248e-05, 'epoch': 0.68}
{'loss': 3.2575, 'learning_rate': 1.5068493150684931e-05, 'epoch': 0.7}
{'loss': 3.2877, 'learning_rate': 1.4383561643835617e-05, 'epoch': 0.71}
{'loss': 3.352, 'learning_rate': 1.3698630136986302e-05, 'epoch': 0.73}
{'loss': 3.2377, 'learning_rate': 1.3013698630136986e-05, 'epoch': 0.74}
{'loss': 3.2754, 'learning_rate': 1.2328767123287671e-05, 'epoch': 0.75}
{'loss': 3.2187, 'learning_rate': 1.1643835616438355e-05, 'epoch': 0.77}
{'loss': 3.3596, 'learning_rate': 1.0958904109589042e-05, 'epoch': 0.78}
{'loss': 3.2139, 'learning_rate': 1.0273972602739726e-05, 'epoch': 0.79}
{'loss': 3.2888, 'learning_rate': 9.589041095890411e-06, 'epoch': 0.81}
{'loss': 3.2764, 'learning_rate': 8.904109589041095e-06, 'epoch': 0.82}
{'loss': 3.3164, 'learning_rate': 8.21917808219178e-06, 'epoch': 0.84}
{'loss': 3.2387, 'learning_rate': 7.5342465753424655e-06, 'epoch': 0.85}
{'loss': 3.2328, 'learning_rate': 6.849315068493151e-06, 'epoch': 0.86}
{'loss': 3.309, 'learning_rate': 6.1643835616438354e-06, 'epoch': 0.88}
{'loss': 3.3016, 'learning_rate': 5.479452054794521e-06, 'epoch': 0.89}
{'loss': 3.1722, 'learning_rate': 4.7945205479452054e-06, 'epoch': 0.9}
{'loss': 3.2523, 'learning_rate': 4.10958904109589e-06, 'epoch': 0.92}
{'loss': 3.265, 'learning_rate': 3.4246575342465754e-06, 'epoch': 0.93}
{'loss': 3.1995, 'learning_rate': 2.7397260273972604e-06, 'epoch': 0.95}
{'loss': 3.26, 'learning_rate': 2.054794520547945e-06, 'epoch': 0.96}
{'loss': 3.3073, 'learning_rate': 1.3698630136986302e-06, 'epoch': 0.97}
{'loss': 3.2782, 'learning_rate': 6.849315068493151e-07, 'epoch': 0.99}
{'loss': 3.2126, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 346.9028, 'train_samples_per_second': 6.682, 'train_steps_per_second': 0.21, 'train_loss': 3.33542156872684, 'epoch': 1.0}
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     3.3354
  train_runtime            = 0:05:46.90
  train_samples            =       2318
  train_samples_per_second =      6.682
  train_steps_per_second   =       0.21
11/22/2023 00:42:53 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3158] 2023-11-22 00:42:53,638 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2023-11-22 00:42:53,638 >>   Num examples = 240
[INFO|trainer.py:3163] 2023-11-22 00:42:53,638 >>   Batch size = 32
/home/fuyali/miniconda3/envs/gpt2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:03<00:11,  1.97s/it] 38%|███▊      | 3/8 [00:07<00:13,  2.79s/it] 50%|█████     | 4/8 [00:11<00:12,  3.22s/it] 62%|██████▎   | 5/8 [00:15<00:10,  3.48s/it] 75%|███████▌  | 6/8 [00:19<00:07,  3.63s/it] 88%|████████▊ | 7/8 [00:23<00:03,  3.72s/it]100%|██████████| 8/8 [00:25<00:00,  3.18s/it]100%|██████████| 8/8 [00:26<00:00,  3.31s/it]
***** eval metrics *****
  epoch                   =        1.0
  eval_accuracy           =      0.414
  eval_loss               =     3.1272
  eval_runtime            = 0:00:30.43
  eval_samples            =        240
  eval_samples_per_second =      7.885
  eval_steps_per_second   =      0.263
  perplexity              =    22.8098
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁
wandb:                      eval/loss ▁
wandb:                   eval/runtime ▁
wandb:        eval/samples_per_second ▁
wandb:          eval/steps_per_second ▁
wandb:                    train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:              train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:            train/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:                     train/loss █▇▆▄▆▄▄▄▄▃▄▄▄▃▃▄▃▃▃▃▄▂▂▃▃▃▁▂▃▂▂▂▃▃▂▃▂▂▃▂
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.41396
wandb:                      eval/loss 3.12719
wandb:                   eval/runtime 30.4377
wandb:        eval/samples_per_second 7.885
wandb:          eval/steps_per_second 0.263
wandb:                    train/epoch 1.0
wandb:              train/global_step 73
wandb:            train/learning_rate 0.0
wandb:                     train/loss 3.2126
wandb:               train/total_flos 1211349860352000.0
wandb:               train/train_loss 3.33542
wandb:            train/train_runtime 346.9028
wandb: train/train_samples_per_second 6.682
wandb:   train/train_steps_per_second 0.21
wandb: 
wandb: 🚀 View run graceful-pine-3 at: https://wandb.ai/161487824/huggingface/runs/c0ox9krr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231122_003707-c0ox9krr/logs
